{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Prompt Word Choice Analysis \n",
    "\n",
    "This notebook contains the code to generate (1) tfidf values and visualizations and (2) tokenize prompt text. \n",
    "\n",
    "\n",
    "The approach for running `TfidfVectorizer` with a bespoke tokenizer was inspired by: https://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
    "\n",
    "Code for generating the by-subset tfidf visualizations is taken directly from: https://buhrmann.github.io/tfidf-analysis.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `interactions.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "\n",
    "prompt_data_raw = pd.read_csv(\"../raw_data/interactions.csv\")\n",
    "prompt_data_raw.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 categories of prompts (`First Success`, `Last Success`, `First Failure`, `Last Failure`) into one column for later looking at `top_feats_by_class`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def create_attempt(row):\n",
    "    if row['is_first_success']:\n",
    "        row['attempt']= \"First Success\"\n",
    "    elif row['is_last_success']:\n",
    "        row['attempt']= \"Last Success\"\n",
    "    elif row['is_first_failure']:\n",
    "        row['attempt']= \"First Failure\"\n",
    "    elif row['is_last_failure']:\n",
    "        row['attempt']= \"Last Failure\"\n",
    "    else: \n",
    "        row['attempt']= \"Other\"\n",
    "    return row\n",
    "\n",
    "prompt_data_raw = prompt_data_raw.apply(create_attempt, axis=1)\n",
    "len(prompt_data_raw)\n",
    "prompt_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to exclude the data appropriately, by sorting out the excluded set of prompts. This code facilitates those changes, as per `compute_pass1.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "excludes_df = pd.read_csv(\"../computed_data/exclude.csv\")\n",
    "\n",
    "excludes_by_problem = excludes_df.groupby(\"problem\").agg({\"text\": list}).reset_index()\n",
    "excludes_by_problem = {row[\"problem\"]: row[\"text\"] for _, row in excludes_by_problem.iterrows()}\n",
    "\n",
    "def string_inclusion_ignore_punctuation(s1, s2):\n",
    "    # Remove punctuation\n",
    "    s1 = s1.translate(str.maketrans('', '', string.punctuation))\n",
    "    s2 = s2.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Check for inclusion and ignore newlines\n",
    "    return s1.strip() in s2\n",
    "\n",
    "def do_exclude(item):\n",
    "    problem = item[\"problem\"]\n",
    "    text = item[\"prompt\"]\n",
    "    if problem in excludes_by_problem:\n",
    "        for exclude in excludes_by_problem[problem]:\n",
    "            if string_inclusion_ignore_punctuation(exclude, text):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply it to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "prompt_data_raw[\"exclude\"] = prompt_data_raw.apply(do_exclude, axis=1)\n",
    "# remove all excluded prompts\n",
    "without_excluded_prompts = prompt_data_raw[~prompt_data_raw[\"exclude\"]]\n",
    "prompt_data = without_excluded_prompts.copy()\n",
    "len(prompt_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not store the parameters for the functions separately in `interactions.csv`, so we need to identify them based on the function signature and then we store them in a data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_args(p):\n",
    "    try: \n",
    "        loc1 = p.index('(')\n",
    "        loc2 = p.index(')')\n",
    "    except:\n",
    "        raise IndexError(\"issue with removing signature from prompt:\", p)\n",
    "    return [x.strip() for x in p[loc1+1:loc2].split(',')]\n",
    "prompt_data[\"args\"] = prompt_data[\"prompt\"].apply(get_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first preprocessing step is to handle different functions' parameter names and function names. We do this prior to tokenization, as once tokenization occurs, the other content in the dataframe is lost. Here we have two functions that do regex-based substitutions using the `sub` method into the `submitted_text` (i.e. prompt) field. We do that for all prompts.\n",
    "\n",
    "We put the replaced terms in `å` (a character not present in the dataset) to help identify them easily in later analyses, and replace them with `*` for presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def arg_helper(arg, val):\n",
    "    current = re.sub(r'([\\[\\s,\\(:=\\-\\+\\\"]|^)' + arg + r'([\\s,\\)\\.\\[\\]\\+:=\\-\\()\\\"]|$)', r'\\1åPARAMå\\2',val)\n",
    "    current = re.sub(\"'\" + arg + \"'\", \"'åPARAMå'\", current) #do not replace posessives, only strings\n",
    "    return current\n",
    "    \n",
    "def replace_args(row):\n",
    "    for arg in row['args']: \n",
    "        row['submitted_text'] = arg_helper(arg, row['submitted_text'])\n",
    "    return row\n",
    "    \n",
    "def replace_func(row):\n",
    "    row['submitted_text'] = re.sub(r'(\\s|^)' + row['problem'] + r'([\\(\\s,]|$)', r'\\1åFUNCTIONNAMEå\\2',row['submitted_text'])\n",
    "    return row\n",
    "\n",
    "def replace_return(row):\n",
    "    row['submitted_text'] = re.sub(r'[rR]eturns', r'åRETURNå',row['submitted_text'])\n",
    "    row['submitted_text'] = re.sub(r'[rR]eturn', r'åRETURNå',row['submitted_text'])\n",
    "    return row\n",
    "\n",
    "prompt_data = prompt_data.apply(replace_func, axis=1)\n",
    "prompt_data = prompt_data.apply(replace_args, axis=1)\n",
    "prompt_data = prompt_data.apply(replace_return, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization for our application (Python terms and English description) was atypical enough and involved enough prioritization that we wrote our own multi-pass regex approach. The `all_together_tokenizer` below takes in a string representing a `submitted text` and outputs a list of tokenized strings of that text. \n",
    "\n",
    "Along the way we also do the preprocessing of making all text lowercase and filtering out stopwords that are not meaningful in this context. There may be possessives and/or contractions that are tokenized as strings rather than separate terms in the dataset.\n",
    "\n",
    "This function was also used for tokenization for the regression analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def all_together_tokenizer(s):\n",
    "    #the set of regexes to match in priority order with () for separators we want to keep around\n",
    "    #(?: ...) groups without keeping around\n",
    "    regexes = [r\"(\\w+(?:\\[[^\\]]*\\])+)\", #regex for list indexing (list[0][1])\n",
    "               r\"(\\[[^\\]]*\\])\", #matches lists and lists of lists\n",
    "               r\"(\\{[^}]*\\})\",  #matches dictionaries\n",
    "               r\"(\\'[^\\']*\\')\", #single quote strings\n",
    "               r\"(\\\"[^\\\"]*\\\")\", #double quote strings\n",
    "               r\"(\\d*\\.\\d+)\",   #floats\n",
    "               r\"([!\\.?,-/():;]+)\", #punctuation that is \"English\" punctuation\n",
    "               r\"(\\d+)\",            #numbers\n",
    "               r\"(=|==|>=|<=|\\-=|\\+=|!=|\\\\n|\\+|\\*|/|\\^|\\<|\\>)\", #comparison operators, math operators, etc.\n",
    "               r\"\\s\"]                          #whitespace \n",
    "    \n",
    "    applied = [] #set of applied regexes\n",
    "    source = [s.lower()] #what we want to return \n",
    "    for elt in regexes: #apply regex in priority order\n",
    "        for i in range(len(source)): #for each of the current strings in the return list\n",
    "            skip = False #flag which determines if we have already matched the string, skip because already applied higher priority regex\n",
    "            for reg in applied: \n",
    "                if re.search(reg, source[i]): #set skip\n",
    "                    skip = True\n",
    "            if not skip:\n",
    "                #from https://www.nltk.org/api/nltk.tokenize.regexp.html\n",
    "                source[i] = re.split(elt, source[i])\n",
    "            else: #if we are keeping it around unchanged, put in a list so flatten works consistently\n",
    "                source[i] = [source[i]]\n",
    "        source = [val for t in source for val in t] #split returns a list, we want one list, not nested lists\n",
    "        applied.append(elt)\n",
    "    #remove some common stop words that are not useful to us - determined collaboratively. \n",
    "    source = filter(lambda x: x not in [\"the\", \"a\", \"an\", \"is\", \"to\", \",\", \".\", \"be\", \"are\", \"at\", \"of\", \"it\", \"as\"], source)\n",
    "    \n",
    "    #replace å with * for better printing\n",
    "    source = map(lambda x: x.replace('å', '*') if \"å\" in x else x, source)\n",
    "\n",
    "    #removes empty strings, https://stackoverflow.com/questions/30933216/split-by-regex-without-resulting-empty-strings-in-python\n",
    "    return list(filter(None, source)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go into a `tf-idf` analysis, first fitting and transforming the model, then making a scatterplot. \n",
    "Note that we use the tokenizer built above rather than a built in tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#this approach for dealing with already tokenized terms for TFIDF \n",
    "#inspired by: https://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    tokenizer=all_together_tokenizer)\n",
    "\n",
    "X = tfidf.fit_transform(prompt_data['submitted_text'].to_numpy().tolist()) \n",
    "print(tfidf.vocabulary_)\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate means across word/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#These mean calculation code was taken directly from: https://buhrmann.github.io/tfidf-analysis.html\n",
    "import numpy as np\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=50):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "print(top_mean_feats(X, tfidf.get_feature_names_out()))\n",
    "success = top_feats_by_class(X, prompt_data[\"is_success\"], tfidf.get_feature_names_out())\n",
    "attempt = top_feats_by_class(X, prompt_data[\"attempt\"], tfidf.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates heatmaps to compare tfidf scores over `First Success`, `First Failure`, `Last Success`, and `Last Failure` subsets. `overall` is the overlapping words and `all` is all words generated in `top_feats_by_class`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set_theme(font_scale=1.2)\n",
    "def plot_heatmap_overlap(dfs):\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.rename(columns ={'tfidf': df.label}, inplace = True)\n",
    "        df.set_index('feature')\n",
    "    combined = dfs[0].merge(dfs[1], how='inner', on=['feature']).merge(dfs[2], how='inner', on=['feature']).merge(dfs[3], how='inner', on=['feature'])\n",
    "    combined = combined.set_index('feature')\n",
    "    combined = combined.sort_index()\n",
    "    plt.subplots(figsize=(7,6))\n",
    "    sns.heatmap(combined, annot=True, fmt='.2f', cmap = \"YlGnBu\")\n",
    "def plot_heatmap_all(dfs):\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.rename(columns ={'tfidf': df.label}, inplace = True)\n",
    "        df.set_index('feature')\n",
    "    concat = pd.merge(dfs[0],dfs[1], on='feature', how='outer').merge(dfs[2], on='feature', how='outer').merge(dfs[3], on='feature', how='outer')\n",
    "    concat = concat.set_index('feature')\n",
    "    concat = concat.sort_index()\n",
    "    plt.subplots(figsize=(10,13))\n",
    "    sns.heatmap(concat, annot=True, fmt='.2f', cmap = \"YlGnBu\")\n",
    "\n",
    "    \n",
    "plot_heatmap_overlap(attempt)\n",
    "plot_heatmap_all(attempt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
